# ğŸ“¦ CSV to Parquet Converter with Validation and Archiving

This script is designed to be used in an AWS Glue Python Shell Job. It reads CSV files from an S3 path, converts them to Parquet (Snappy compression), validates the conversion, and optionally moves the original CSVs to an archive location.

## âœ… Features
	â€¢	Converts .csv files to .parquet format (with Snappy compression)
	â€¢	Automatically detects and converts date/time columns
	â€¢	Validates row count and column names between CSV and Parquet
	â€¢	Archives original CSV files if enabled

## ğŸ“ S3 Structure
	â€¢	Input path: s3://<bucket>/<table_name>/date=YYYYMMDD/<file>.csv
	â€¢	Output path: s3://<bucket>/<table_name>/date=YYYYMMDD/<file>.snappy.parquet
	â€¢	Archive path: s3://<bucket>/<table_name>_archive/date=YYYYMMDD/<file>.csv

## ğŸ§ª Example Job Arguments
    --BUCKET_NAME bb2-sandbox-datalake-raw
    --TABLE_NAMES salesforce_account
    --ARCHIVE_ENABLED true

## ğŸš€ Execution Steps
	1.	Initialize logger
	2.	List all .csv files under the given S3 path
	3.	For each file:
	â€¢	Convert to .snappy.parquet
	â€¢	Attempt to auto-convert timestamp columns
	â€¢	Validate structure (row count & columns)
	â€¢	Archive CSV if enabled

â¸»

## ğŸ§± Dependencies
	â€¢	awswrangler
	â€¢	boto3
	â€¢	pandas
	â€¢	aws-glue-utils (built-in for Glue Python shell)

â¸»

## ğŸ“ Notes
	â€¢	Ensure proper IAM permissions are in place for read/write/delete on S3.
	â€¢	This script expects a folder structure that includes date= partitions.
	â€¢	Itâ€™s best to test with sample files before running on production data.
